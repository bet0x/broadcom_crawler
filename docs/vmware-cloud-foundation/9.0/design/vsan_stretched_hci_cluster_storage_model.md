---
source_url: https://techdocs.broadcom.com/us/en/vmware-cis/vcf/vcf-9-0-and-later/9-0/design/design-library/storage-models(1)/standard-vsan-storage-model/stretched-cluster-storage-models/stretched-vsan-storage-model.html
product: vmware-cloud-foundation
version: 9.0
section: Design
breadcrumb: Design > vSAN Stretched HCI Cluster Storage Model
---

# vSAN Stretched HCI Cluster Storage Model

A vSAN Stretched HCI Cluster Storage Model is a cluster configuration where the ESX hosts, that comprise a single vSAN cluster, reside in two availability zones.

vSAN Stretched HCI Cluster Storage Storage Model

vSAN Stretched HCI Cluster Storage Model depicting three availability zones for vSAN . Two availability zones are designated for vSAN data nodes and a third availability zone is designated for a vSAN witness node

![](/content/broadcom/techdocs/us/en/vmware-cis/vcf/vcf-9-0-and-later/9-0/_jcr_content/assetversioncopies/ce2a3769-f919-4b3c-912c-b07b4f98c5d2.original.svg)

vSAN Stretched Cluster Storage Model Attributes



| Attribute | Detail |
| --- | --- |
| Fault domain / availability zone | - vSAN Stretched Clusters comprise of three (3) Fault domains. A fault domain comprises of one or more vSAN nodes. - A fault domain or availability zone can represent a physical location. Fault domains / availability zones can either be two distinct data centers in a metro distance, or two safety or fire sectors (data halls) in the same large-scale datacenter. |
| vSAN traffic type | - vSAN Stretch Clusters utilize vSAN Traffic type for data traffic. - vSAN Stretch Clusters utilize vSAN Witness Traffic type for vSAN meta-data traffic.   vSAN Stretch Storage Clusters (ESA) can be configured to use vSAN Storage Cluster Client traffic for vSAN compute client clusters. See [vSAN Stretched Storage Cluster Storage Model](/us/en/vmware-cis/vcf/vcf-9-0-and-later/9-0/design/design-library/storage-models(1)/standard-vsan-storage-model/stretched-cluster-storage-models/vsan-stretched-storage-cluster.html). |
| vSAN witness appliance | A witness node is a pre-configured appliance that runs ESX and is distributed as an OVA file. |

vSAN Stretched HCI Cluster Storage Model Options



| Design Area | Options |
| --- | --- |
| Storage models | - [vSAN ESA Storage Model](/us/en/vmware-cis/vcf/vcf-9-0-and-later/9-0/design/design-library/storage-models(1)/standard-vsan-storage-model/single-rack-storage-models/vcf-hardware-configuration-for-vsan.html) - [vSAN OSA Storage Model](/us/en/vmware-cis/vcf/vcf-9-0-and-later/9-0/design/design-library/storage-models(1)/standard-vsan-storage-model/single-rack-storage-models/vsan-osa-storage-model.html) |
| Witness traffic separation | - Witness traffic separation allows you to use a distinct VMkernel adapter for vSAN witness traffic communication with the vSAN witness appliance. - There are two choices when stretching a vSAN clusters.    - When enabled, the ESX management network is used to communicate with the vSAN witness node via the management network.   - When disabled, The vSAN data traffic network is used to communicate with the vSAN witness node. This requires routing the vSAN data network VLAN to the vSAN witness network. |

Common vSAN Design Requirements for All Models



| Design Requirement ID | Design Requirement | Justification | Implication |
| --- | --- | --- | --- |
| VCF-VSAN-REQD-CFG-001 | Minimum vSAN cluster size. | Minimum valid vSAN HCI cluster is three (3) nodes all contributing to storage. | - Supports simple install of a management domain default vSphere cluster only. - A three (3) node vSAN cluster will not offer automatic rebuild of components in the event of a failure. - Using a smaller cluster limits the workload that can be placed on a vSAN cluster.   vSAN Storage Clusters require minimum four (4) vSAN Nodes see [vSAN Single-Rack Storage Cluster Storage Model](/us/en/vmware-cis/vcf/vcf-9-0-and-later/9-0/design/design-library/storage-models(1)/standard-vsan-storage-model/single-rack-storage-models/vsan-storage-cluster-storage-model.html) for more details |
| VCF-VSAN-REQD-CFG-002 | Provide sufficient raw capacity to meet the initial needs of the workload domain vSphere cluster. | Ensures that sufficient resources are present to create the workload domain vSphere cluster. | Requires determining the workload capacity requirements prior to deployment.  Refer to the [VMware Cloud Foundation Planning and Preparation Workbook](https://techdocs.broadcom.com/content/dam/broadcom/techdocs/us/en/assets/vmware-cis/vcf/VMware%20Cloud%20Foundation%205.2%20Planning%20and%20Preparation%20Workbook.xlsx) for accurate sizing guidance. |
| VCF-VSAN-REQD-CFG-003 | vSphere Lifecycle Management (vLCM) image based management is required for vSAN clusters. | - All new vSAN clusters, irrespective of OSA or ESA architectures, are required to be using vSphere Lifecycle Manager images for VMware Cloud Foundation. - As per VCF-CLS-REQD-CFG-003 vSphere Lifecycle Manager images simplify the management of firmware and vendor add-ons. | Imported vSAN OSA clusters may use vLCM baselines, but have to be converted prior to upgrade to VMware Cloud Foundation 9.0. |
| VCF-VSAN-REQD-CFG-004 | Verify the hardware components used for vSAN deployments are on the vSAN hardware compatibility list. | Helps prevent hardware related issues during workload deployment and operation. | - Requires validating vSAN hardware to ensure its on the compatibility list prior to deployment. - Restricts the number of devices that can be used to a certified list.   Refer to the vSAN hardware compatibility guide for certified hardware [https://compatibilityguide.broadcom.com/.](https://compatibilityguide.broadcom.com/) |
| VCF-VSAN-REQD-CFG-005 | Do not use NVMe storage devices attached to a Tri-Mode controller for vSAN diskgroups or storage pools. | - Discrete RAID controllers that support SATA/SAS/NVMe are often known as "Tri-mode controllers". - VMware vSAN will **only** support SAS and SATA devices attached to a Tri-mode controller. - NVMe devices attached to Tri-mode controller is **NOT** a vSAN supported configuration.  Refer to knowledge base article [KB314305](https://knowledge.broadcom.com/external/article/314305/vsan-support-of-nvme-devices-behind-trim.html) | - NVMe devices are only supported directly connected to a PCIe slot on the bus. - May require to work with server vendor to verify or modify configurations. |
| VCF-VSAN-REQD-CFG-006 | For vSAN HCI clusters, excluding vSAN storage clusters, configure the vSAN network gateway IP address as the isolation address for the cluster. | vSphere HA can validate against an IP address on the vSAN network if a host is isolated. | Allocate an additional IP address. Top of rack switches may configure a port for switch virtual interface (SVI) which can be used as the isolation address.  See VCF-VSAN-NET-REQD-CFG-003 |
| VCF-VSAN-REQD-CFG-007 | For vSAN HCI clusters, excluding vSAN storage clusters, set the advanced cluster setting das.usedefaultisolationaddress to false. | vSphere HA in vSAN uses the ESX hosts management network default gateway IP for isolation detection when configured with default settings. This setting avoids conflicts if vSphere HA and vSAN triggers different responses when failures occur. | May require to configure a SVI (Switched Virtual Interface) gateway address for the VLAN assigned to the vSAN network. |

vSAN Stretched Cluster Model Design Requirements



| Design Requirement ID | Design Requirement | Justification | Implication |
| --- | --- | --- | --- |
| VCF-VSAN-SC-REQD-CFG-001 | Two (2) availability zones and a third site for the witness appliance location. | Fault domains are mapped to availability zones to provide logical and physical ESX host separation and ensure a copy of vSAN data is always available even when an availability zone goes offline. | - This requires both availability zones to have an equal number of ESX hosts residing in each site, and are connected using a high bandwidth/low latency connection known as an inter-site link, or ISL. - The third site should host the witness appliance and must not have a logical or physical dependency on either availability zone. |
| VCF-VSAN-SC-REQD-CFG-002 | The minimum latency required between the data sites hosting virtual machine objects should not exceed 5 milliseconds (ms) in round trip time (RTT), or 2.5ms one way. | The latency minimums exist largely to ensure that applications can write data resiliently across two sites in a timely manner. | Stretched cluster network latency requirements will dictate the realistic geographic distance between sites. |
| VCF-VSAN-SC-REQD-CFG-003 | The minimum latency required between witness site and the data sites should not exceed 200ms RTT, or 100ms one way. | The witness site only stores small amounts of metadata, and is not a part of the vSAN data path. | None. |
| VCF-VSAN-SC-REQD-CFG-004 | vSAN Stretched Cluster Data Site to Data Site Bandwidth minimum is no less than 10GbE.  - The vSAN Stretched Cluster Bandwidth Sizing document can be used for general guidance in sizing for vSAN OSA and ESA architectures. | The network requirements for the data-site to data site in a stretch cluster remain the same for both vSAN ESA and OSA. | - VM workloads will ultimately determine the network bandwidth demand between data-sites.  - Please review <https://www.vmware.com/docs/vmw-vsan-stretched-cluster-bandwidth-sizing> |
| VCF-VSAN-SC-REQD-CFG-005 | vSAN Stretched Cluster Data Site to Witness Site Bandwidth is based on number of VMs that is protected by a stretch cluster storage policy. | - Network communication from the data sites to the witness host appliance is comprised entirely of metadata and this is wholly dependent on the number of VM and components protected by vSAN Stretch Cluster polices.   - There is no minimum bandwidth defined as estimating the bandwidth required for the witness site is based on the number of VM components. | For accurate sizing please review <https://www.vmware.com/docs/vmw-vsan-stretched-cluster-bandwidth-sizing> |
| VCF-VSAN-SC-REQD-CFG-006 | Management Domain initial cluster  Minimum of eight (8) nodes for vSAN stretched cluster for high availability (HA) install | Management domain initial clusters deployed in high availability (HA) mode have a requirement for 4 nodes in a non-stretched deployment. Stretching a cluster is a day two operation and will require 4 additional hosts on the second availability zone. | vSAN stretched cluster will operate optimally with a balanced ESX host configuration on each availability zone and be able to tolerate a local site failure |
| VCF-VSAN-SC-REQD-CFG-007 | Workload domain clusters (except for management Domain initial cluster)  Minimum of six (6) nodes for vSAN stretched clusters | This will provide minimum requirements for site availability and site local availability, tolerating a site failure and one failure on each availability zone.  This will give a balance between a host failure on a local availability zone and a full availability zone failure. | vSAN stretched cluster will operate optimally with a balanced ESX host configuration on each availability zone and be able to tolerate a local site failure with minimum three (3) nodes per site.  However with three nodes per site, vSAN will not automatically rebuild objects that are impacted by a local failure. |
| VCF-VSAN-SC-REQD-CFG-008 | For vSAN OSA architecture  Modify the default Storage Policy as follows   - **Site disaster tolerance:** Site mirroring - **Failure to tolerate :** One (1) failure RAID-1 (Mirroring) | Provides the minimum necessary protection for VMs in each availability zone, with the ability to recover from an availability zone outage.  - For vSAN ESA , Auto Policy configuration will automatically alter the default storage policy to reflect the number of hosts and underlying vSAN topology, when the cluster is converted to a stretch cluster. | - Additional Storage is necessary to satisfy the storage policy requirements in both availability zones. - Additional policies can be configured for specific workloads depending on their individual performance or availability requirements which may differ from what the default vSAN stretched cluster policy. |
| VCF-VSAN-SC-REQD-CFG-009 | Deploy a vSAN witness appliance in a location that is not co-located to the ESX hosts in any of the availability zones. | Ensures availability of vSAN witness appliance in the event of a failure of one of the availability zones. | You must provide a third availability zone that is logically and physically independent of the data sites  The third site must be able to provide a environment capable to host the witness appliance. |
| VCF-VSAN-SC-REQD-CFG-010 | Deploy a witness appliance that corresponds to vSAN Architecture, (ESA or OSA). | Ensures the witness appliance deployed is compatible to the correct vSAN Storage architecture | None. |
| VCF-VSAN-SC-REQD-CFG-011 | Connect the first VMkernel (vmk0) adapter of the vSAN witness appliance to the management network in the witness site. | The ESX host management networks in both availability zones must be routed to the vSAN witness host management network in the third witness site. | When using witness traffic separation, the second interface on vSAN witness host (vmk1) can be removed. |
| VCF-VSAN-SC-REQD-CFG-012 | Configure vSAN Witness host identity as per standard ESX host recommendations. | - Allows vSAN Witness host to be managed identically as a ESX host.  - Ensures host identity, DNS and NTP is consistently configured | Apply the requirements for [External Services Design](/us/en/vmware-cis/vcf/vcf-9-0-and-later/9-0/design/design-library/vcf-vcenter-server-networking.html#GUID-c17bff55-9bff-4181-be36-b0bc39c59133-en_GUID-FF48E31A-A83D-4EED-8FFC-9A2B4F0EB331) |
| VCF-VSAN-SC-REQD-CFG-013 | Enable vSphere Lifecycle Manager images on the vSAN witness host. | - VMware Cloud Foundation 9.0 requires vSphere Lifecycle Manager images for ESX hosts. - This will prevent warnings or vSAN health check alarms. | This requires either deploying or converting a vSAN witness host as a vSphere Lifecycle Manager image managed entity. |

Common vSAN Network Design Requirements



| Design Requirement ID | Design Requirement | Justification | Implication |
| --- | --- | --- | --- |
| VCF-VSAN-NET-REQD-CFG-001 | vSAN requires a high speed, low latency network for synchronous replication of vSAN components.  vSAN can use 10GbE , 25GbE or greater. | 10GbE is the minimum network required for vSAN (ESA and OSA architectures  There is limited support for 10GbE and vSAN ESA.  For vSAN ESA ready node, **vSAN-ESA-AF-0** profile, only supports 10GbE. Please refer to <https://partnerweb.vmware.com/comp_guide2/vsanesa_profile.php> | - Using 10GbE for vSAN networking can **severely** limit vSAN performance and can introduce bandwidth constraints combined with high latency. - Larger vSAN ESA nodes require 25GbE.   Using 10GbE networking on vSAN ESA will trigger a vSAN cluster compliance check.  Please review Broadcom Knowledge base [KB372309](https://knowledge.broadcom.com/external/article/372309/workaround-to-reduce-impact-of-resync-tr.html) |
| VCF-VSAN-NET-REQD-CFG-002 | - Use a dedicated VLAN for a single rack cluster You can share a single VLAN for multiple vSAN clusters within the same rack  - For stretched cluster topology, use a dedicated VLAN per availability zone. See [vSAN Stretched Cluster Model Design](/us/en/vmware-cis/vcf/vcf-9-0-and-later/9-0/design/design-library/storage-models(1)/standard-vsan-storage-model/stretched-cluster-storage-models/stretched-vsan-storage-model.html) - For vSAN HCI multi-rack topology, VLAN requirements will depend on [vSphere Multi-Rack Cluster Detailed Design](/us/en/vmware-cis/vcf/vcf-9-0-and-later/9-0/design/design-library/cluster-models/multi-rack-cluster-detailed-design.html) if using L2 or L3 design | - For a single rack cluster using a dedicated VLAN allows for better storage traffic isolation and security.  - For vSAN Stretched Cluster VCF automated workflows, it is required to have two (2) VLANs, one VLAN per availability zone. | - For a single rack Cluster a VLAN ID must be allocated for use for a vSAN Cluster.  - For a vSAN Stretch Cluster topology two VLAN IDs must be allocated for use in a stretched cluster topology. These VLAN must be routable to each other - For vSAN HCI multi-rack topology, VLAN requirements will depend on the deployment, if using L2 or L3 design |
| VCF-VSAN-NET-REQD-CFG-003 | - Use one (1) IPv4 IP address subnet for vSAN traffic in a single rack cluster. - For vSAN stretch cluster topology, use two (2) IPv4 subnets, see [VLAN and IP Subnet Requirements for vSAN Stretched Cluster in a VCF Environment](/us/en/vmware-cis/vcf/vcf-9-0-and-later/9-0/design/design-library/storage-models(1)/standard-vsan-storage-model/stretched-cluster-storage-models/vsan-stretched-cluster-network-requirements.html) | The vSAN VMkernel in a VMware Cloud Foundation platform requires IPv4 addressing. | - You require an IPv4 subnet configured with a correctly sized subnet to support the number of free IP addresses for each vSAN node in a single rack vSAN Cluster. - A gateway IP is required. |

Common vSAN Design Recommendations for All Models



| Design Recommendation ID | Design Recommendation | Justification | Implication |
| --- | --- | --- | --- |
| VCF-VSAN-RCMD-CFG-001 | It is recommended to start with four (4) vSAN nodes. | - This ensures vSAN objects will automatically rebuild in the case of a failure or maintenance event. - This allows enabling Reserved Capacity on four (4) node clusters. - This allows space efficient storage polices to be automatically configured when used with vSAN ESA clusters. | This will potentially increase cost and hardware footprint of vSAN cluster. |
| VCF-VSAN-RCMD-CFG-002 | Include TPM (Trusted Platform Module) in the hardware configuration of the ESX hosts. | Ensures that the keys issued to the hosts in a vSAN cluster using Data-at-Rest Encryption are cryptographically stored on a TPM device. | Can limit the choice of hardware configurations. |
| VCF-VSAN-RCMD-CFG-003 | Enable Reserved Capacity on vSAN clusters. | - Reserved Capacity will implement a sliding scale storage capacity resource reservation as the vSAN node count increases per vSAN cluster. For example, the Host Rebuild Reserve for a four (4) node cluster would be 25% while it would be just 8% for a twelve (12) node cluster. This avoids imposing a generic 30% capacity overhead of a vSAN Cluster, thus dynamically optimizing reserved storage capacity on vSAN. - Recommendation is to design for vSphere cluster host counts large enough to lower the percentage needed for Reserved Capacity, This will provide the most efficient, yet agile vSphere cluster design. | - When enabled vSAN will impose two soft limits: - **Operations Reserve** - limits for capacity needed to perform transient storage activities like storage policy changes, re-balancing. - **Host Rebuild Reserve** - will reserve the capacity needed to absorb a sustained failure of a single ESX host in a vSAN cluster â€“ to support an N+1 cluster design strategy. - "**Host Rebuild Reserve**" has the following implications in a vSAN storage cluster that consist of only 4 hosts. When paired with the Auto-Policy Management feature, this will prevent vSAN storage clusters from being able to use space-efficient and more performant RAID-5 policy.   vSAN capacity reserve is not supported in conjunction with the following vSAN Cluster typologies:  - Three (3) node clusters. - vSAN stretched clusters. - vSAN clusters using fault domains. |
| VCF-VSAN-RCMD-CFG-004 | Enable disk Automatic Re-balance. | - When enabled vSAN will attempt to automatically keep the storage consumption of each capacity drive within a default variance threshold of 30%. - This allows vSAN to more evenly distribute the data across the discrete devices to achieve a balanced distribution of resources, and thus, improved performance. | - Adding or scaling out new hosts will trigger a disk rebalance and resynchronization. - When adding or replacing failed resources, automatic rebalance may introduce additional data re-synchronization and as a result additional network traffic utilization  - re-synchronization may have an impact when vSAN clusters are network bandwidth constrained. |

vSAN Stretched Cluster Model Design Recommendations



| Design Recommendation ID | Design Recommendation | Justification | Implication |
| --- | --- | --- | --- |
| VCF-VSAN-SC\_RCMD-001 | It is recommend that vSAN Stretched cluster topological use Layer 3 (routed) networking between the two data sites, and the witness site.   - The data sites will only be able to communicate with each other using the ISL - The witness host should only be able to communicate with hosts in each data site directly, and not through another data site. | Layer 3 will help avoid spanning tree protocol (STP) from redirecting traffic across an undesirable link, such as the more bandwidth constrained link to the witness site | Ensure Layer 3 gateways are highly available such that it tolerates the failure of an entire availability zone. |
| VCF-VSAN-SC\_RCMD-001 | Use witness traffic separation when stretching a vSAN cluster. | - **Network Flexibility:** You can configure different VLANs or subnets for vSAN data and witness traffic, allowing for better network segmentation and resource allocation.  - **Mixed MTU:** This will allow vSAN witness traffic type to use a different MTU size than the vSAN data traffic. This avoids configuring static routes between vSAN networks.  - **Simplified Troubleshooting:** Identifying and resolving network issues related to witness traffic becomes easier when it is isolated from the main vSAN data traffic. | This requires the vSAN witness network to be routable to both the ESX host management networks on either availability zone as well to VM management network if configured. |
| VCF-VSAN-SC-RCMD-CFG-002 | Configure the vSAN witness host to use the first VMkernel adapter (vmk0), that is the management interface, for vSAN witness traffic. | - Removes the requirement to have two IP addresses allocated to the vSAN witness host. - Removes the requirement to have static routes in the witness as the witness traffic is routed over the management networks | - The management networks in both availability zones must be routed to the management witness site.  - Ensure to use the same MTU size as the host management interfaces.   Default MTU size is 1500 for management interfaces |
| VCF-VSAN-SC-RCMD-CFG-003 | Configure vSAN Storage policies to use "Force Provisioning" attribute. | This will allow for vSAN objects to be created in the event of an Availability zone being unavailable due to site maintenance or failure. | This would require modifying vSAN policies used as part of the stretched cluster. |
| VCF-VSAN-SC-RCMD-CFG-004 | Ensure DRS Host Groups, VM Groups, and VM/Host Rules are in conjunction with vSAN storage policies. | - This ensures that VM and vSAN data resilience desired outcomes are satisfied. - This will suggest or force a behavior of VM placement in a vSAN stretched cluster while in steady state. - This ensures vSphere HA will trigger the desired behavior in response to a failure. | - Requires creation of additional DRS Host groups, VM Groups, and VM/Host DRS Rules. - The Placement rules should match the desired failure response. |
| VCF-VSAN-SC-RCMD-CFG-005 | vSphere HA Admission control fail-over capacity is configured to reserve 50% of the CPU and memory capacity of the entire stretched cluster. | This ensures that VMs can fail between AZs and have adequate resources. | Requires adequate capacity guarantee successful fail-over of protected workloads in the event of failure of an entire availability zone. |
| VCF-VSAN-SC-REQD-CFG-006 | Do not enable reserved Capacity in a vSAN stretched Cluster. | vSAN capacity reserve is not supported in conjunction with the following vSAN cluster topologies:   - Three (3) node clusters. - vSAN stretched clusters. - vSAN clusters using fault domains. | This will require manually calculating vSAN slack space.  Refer to the vSAN sizer for recommended capacity at <https://vcf.broadcom.com/tools/vsansizer/home.> |